{"short":"<p>Multimodal zero-shot learning extends and combines the ideas behind\nobject detection and image embedding. The goal is to identify a\nsimilarity score between any pair of a textual description and an image\nsuch that the score is large when the description serves as a good\ncaption for the image. This allows us to search for any categories\n(however complex) without having to create a specialized model for it.\nHere, we first run an embedding over the entire collection and then on\nthe right allow for inputing a search string. We suggest starting with\nthe large example datasets as the have pre-computed embeddings and\nquickly show the powerful possibilities of this approach.<\/p>","long":"<p>Multimodal zero-shot learning extends and combines the ideas behind\nobject detection and image embedding. The goal is to identify a\nsimilarity score between any pair of a textual description and an image\nsuch that the score is large when the description serves as a good\ncaption for the image. This allows us to search for any categories\n(however complex) without having to create a specialized model for it.\nHere, we first run an embedding over the entire collection and then on\nthe right allow for inputing a search string. We suggest starting with\nthe large example datasets as the have pre-computed embeddings and\nquickly show the powerful possibilities of this approach.<\/p>\n<h3 id=\"references-12\">References<\/h3>\n<pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training}, \n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}<\/code><\/pre>"}
