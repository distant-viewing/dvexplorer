{"short":"<p>For photography, depth estimation identifies the relative distance of\neach pixel in the image away from the camera. We can extend this to\nother types of images by considering the objects' depths from the\nartist's perspective. The output will show the closest pixels in white\nand the farthest pixels in black. Hoving over the production reveals the\noriginal image for comparison. Downloading the results will provide a\nzipped directory of all the depth-predicted images.<\/p>","long":"<p>For photography, depth estimation identifies the relative distance of\neach pixel in the image away from the camera. We can extend this to\nother types of images by considering the objects' depths from the\nartist's perspective. The output will show the closest pixels in white\nand the farthest pixels in black. Hoving over the production reveals the\noriginal image for comparison. Downloading the results will provide a\nzipped directory of all the depth-predicted images.<\/p>\n<h3 id=\"references-2\">References<\/h3>\n<pre><code>@misc{yang2024depth,\n      title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, \n      author={Lihe Yang and Bingyi Kang and Zilong Huang and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},\n      year={2024},\n      eprint={2401.10891},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}<\/code><\/pre>"}
