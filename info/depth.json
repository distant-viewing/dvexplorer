{"short":"<p>For photography, depth estimation is the task of identifying the\nrelatively distance of each pixel in the image away from the camera. We\ncan extend this to other types of images by considering the depths of\nobjects from the perspective of the artist. The output will show the\nclosest pixels in white and the farthest pixels in black. Hoving over\nthe output reveals the original image for comparison. Downloading the\nresults will provide a zipped directory of all the depth-predicted\nimages.<\/p>","long":"<p>For photography, depth estimation is the task of identifying the\nrelatively distance of each pixel in the image away from the camera. We\ncan extend this to other types of images by considering the depths of\nobjects from the perspective of the artist. The output will show the\nclosest pixels in white and the farthest pixels in black. Hoving over\nthe output reveals the original image for comparison. Downloading the\nresults will provide a zipped directory of all the depth-predicted\nimages.<\/p>\n<h3 id=\"references-2\">References<\/h3>\n<pre><code>@misc{yang2024depth,\n      title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data}, \n      author={Lihe Yang and Bingyi Kang and Zilong Huang and Xiaogang Xu and Jiashi Feng and Hengshuang Zhao},\n      year={2024},\n      eprint={2401.10891},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}<\/code><\/pre>"}
